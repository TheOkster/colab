{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Training your own model with Tensorflow!"
      ],
      "metadata": {
        "id": "V8smNaCo4E5w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This exercise will done using Google Colab, Python, and Tensorflow.\n",
        "\n"
      ],
      "metadata": {
        "id": "SsaW3DLi9Oif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please click on the play button below to make sure you have the necessary libraries."
      ],
      "metadata": {
        "id": "PjcY0jZY4UNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install matplotlib\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ],
      "metadata": {
        "id": "9oNu3KS64TI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lesson, we'll be classifying movie reviews as negative or positive based off IMDB's dataset! I don't know how many of you have heard of IMDB, but it's a popular movie database website where users can rate movies.\n",
        "\n",
        "Press the button below to download the dataset onto this Colab instance! It won't download to your actual machine.\n",
        "It may take 5s-1m to download."
      ],
      "metadata": {
        "id": "HqxhqPwp5t4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
        "print(f\"Training Sample Count: {len(x_train)}\")\n",
        "print(f\"Test Sample Count: {len(x_test)}\")"
      ],
      "metadata": {
        "id": "8TjjgoSg6PE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset has been split into train & test subsets.\n",
        "The train subset is used to build up the model. The test subset is not used to make the model directly, but rather to test if it's accurate.\n",
        "\n",
        "You should always have a train & test dataset to make sure your data is accurate. In this case, 50% of the data is used for testing and 50% is used for training."
      ],
      "metadata": {
        "id": "u9L6CR3E9kSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make the dataset in a format that Tensorflow can use, we need to play around with the text in the dataset. This command makes sure that no review is more than 256 characters.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0AyvRoA4-jQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = pad_sequences(x_train, maxlen=256)\n",
        "x_test = pad_sequences(x_test, maxlen=256)\n"
      ],
      "metadata": {
        "id": "to2NDFk--q8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's actually create the model!"
      ],
      "metadata": {
        "id": "WGO63Oxm_N9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=10000, output_dim=64, input_length=256),\n",
        "    Flatten(),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "Y_hY319W_gAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This might be a bit difficult to understand.\n",
        "When words are input into this model, they aren't actually recognized by the model as words, but numbers. This allows it to more easily group words with similar meaning together e.g. \"mad\" and \"angry\" together to make it easier for the model to recognize.\n",
        "\n",
        "Flatten and Dense are outside of scope of this lesson to explain, but feel feel free to look them up if interested!"
      ],
      "metadata": {
        "id": "f6fKVuEKERjx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll train the model using the training data we downloaded earlier! This may take a while. If you want it to go faster, you can decrease the number of epochs (min times it runs through every value in the dataset) but that may affect accuracy!"
      ],
      "metadata": {
        "id": "ox26voqF_sWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=4, batch_size=32, validation_data=(x_test, y_test))\n"
      ],
      "metadata": {
        "id": "CVuou3ua_vLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our model is trained, it's time to see if it can correctly classify our test data!\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X52dXFkH_y30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(f'Accuracy: {round(test_accuracy*100, 2)}%')\n"
      ],
      "metadata": {
        "id": "j5ilrPhp_3Ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy will change each time you complete the sequence, but for me it was 85.7% accurate. Not bad!\n",
        "\n",
        "Don't focus on the loss value for now; it measures how well the model's predictions fit their regular values. Unlike accuracy, it isn't a percentage, so don't think of it that way and you can ignore it for the purposes of this exercise."
      ],
      "metadata": {
        "id": "vuiNbR2KA3aD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model In Action"
      ],
      "metadata": {
        "id": "5SZ-YgVhBgTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
        "\n",
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = {value: key for key, value in word_index.items()}\n",
        "\n",
        "def decode_review(encoded_review):\n",
        "    return \" \".join([reverse_word_index.get(i - 3, \"?\") for i in encoded_review])\n",
        "\n",
        "def visualize_prediction(review, actual_label, model):\n",
        "    padded_review = pad_sequences([review], maxlen=256)\n",
        "    prediction = model.predict(padded_review)[0][0]\n",
        "    actual_label = \"Positive\" if actual_label == 1 else \"Negative\"\n",
        "    predicted_label = \"Positive\" if prediction > 0.5 else \"Negative\"\n",
        "\n",
        "    print(f\"Review: {decode_review(review)[:257]}{'...' if len(decode_review(review)) > 256 else ''}\")\n",
        "    print(f\"Actual: {actual_label}\")\n",
        "    print(f\"Predicted: {predicted_label}\")\n",
        "    print(f\"Confidence: {round(prediction, 2)*100}% chance it's Positive\")\n",
        "\n",
        "for i in range(5):\n",
        "    visualize_prediction(x_test[i], y_test[i], model)\n",
        "    print('\\n')\n"
      ],
      "metadata": {
        "id": "nM5u6HwbBlj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As mentioned before, the text is actually processed as numbers rather than words directly. The decode function converts the text back into words so humans can understand it.\n",
        "The confidence represents the percentage the model is sure the prediction is positive."
      ],
      "metadata": {
        "id": "oSBdFfv6DyVw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Type your own review here and see how the model categorizes it!"
      ],
      "metadata": {
        "id": "suTOHVTVhfIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_custom_text(model, text):\n",
        "    tokens = [word_index.get(word, 2) + 3 for word in text.lower().split()]\n",
        "    padded_tokens = pad_sequences([tokens], maxlen=256)\n",
        "\n",
        "    prediction = model.predict(padded_tokens)[0][0]\n",
        "    predicted_label = \"Positive\" if prediction > 0.5 else \"Negative\"\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Predicted Label: {predicted_label}\")\n",
        "    print(f\"Confidence: {round(prediction, 2)*100}% it's Positive\")\n",
        "\n",
        "user_text = input(\"Type your own review: \")\n",
        "predict_custom_text(model, user_text)\n"
      ],
      "metadata": {
        "id": "tPP0KlrrDzb9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}